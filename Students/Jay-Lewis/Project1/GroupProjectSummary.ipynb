{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Classification of Natural vs. Computer Generated Images*\n",
    "\n",
    "## Authors\n",
    "* Yixiao Feng\n",
    "* Jyothsna Kurra\n",
    "* Justin Lewis\n",
    "* Tim Woodbury"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Overview:\n",
    "\n",
    "* Two sets of training data given: Natural (\"scenes\") and computer generated (\"synthetic\")\n",
    "* Model-based classification\n",
    "* Generalized detection approach:\n",
    "    * Identify appropriate features\n",
    "    * Train models for the features\n",
    "    * Validate selected models against the provided data sets\n",
    "    \n",
    "Four strategies are considered for discriminating scenes and synthetic images.\n",
    "These are as follows: (1) edge detection; (2) grayscale intensity; (3) RGB peak density; (4) image sharpness.\n",
    "The strategies are each discussed in one of the following sections, in which both the motivation for a particular strategy and results are shown.\n",
    "A final section offers a brief comparison of the performance of the four detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I) Edge Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge detection is considered to distinguish synthetic and natural images.\n",
    "Natural images, which have greater sources of distortion and imperfect lighting, are expected to have a greater number of pixels detectable as edges.\n",
    "This section will present the basic approach used including the training of models for hypothesis testing.\n",
    "Subsequently the likelihood ratio test for image discrimination is presented and the correct classification rate is discussed.\n",
    "\n",
    "#### Approach\n",
    "Gradient-based edge detection is used.\n",
    "Pixels for which the gradient exceeds a fixed threshold are classified as edges.\n",
    "The threshold is determined using a training set of 23 each synthetic and natural images.\n",
    "The following figure shows normalized histograms of the pixel gradients across all images in the data sets.\n",
    "Based on the histogram, a threshold value of 4 was chosen.\n",
    "All pixels whose gradients exceed the threshold are classified as edges.\n",
    "\n",
    "<img style=\"float: left;\" src=\"img1.png\">\n",
    "\n",
    "The threshold is applied to the same training set.\n",
    "For uniformity, each image is resized to $(540 \\times 960)$ before it is processed, ensuring that differences in edge counts are not due to differences in size.\n",
    "The number of edge pixels, as determined by the threshold on the gradient, is then totalled.\n",
    "Applying this metric to the synthetic and natural images separately, a histogram for the number of edge pixels in the training images is obtained.\n",
    "These histograms are shown in the following figure.\n",
    "\n",
    "<img style=\"float: left;\" src=\"img2.png\">\n",
    "\n",
    "The histograms are coarse because of the small size of the training sets.\n",
    "However, they provide a means for determining a plausible probability density function (PDF) for the synthetic and natural image sets.\n",
    "The histograms have very wide tails, so Cauchy distributions are fit to the data.\n",
    "The Cauchy distributions for natural and synthetic images have the following parameters:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathrm{scenes}: x_0 = 131700, \\gamma = 52984.7 \\\\\n",
    "\\mathrm{synthetic}: x_0 = 35760.8, \\gamma = 5377.61\n",
    "\\end{align}\n",
    "\n",
    "Here, $x_0$ indicates the mean and $\\gamma$ the scale parameter.\n",
    "Having determined approximate distributions for the number of edges in the synthetic and natural images, it is a straightforward matter to apply a likelihood ratio test for a new candidate image to classify it as synthetic or natural.\n",
    "\n",
    "#### Likelihood ratio test and performance\n",
    "\n",
    "The likelihood ratio test compares the lieklihood of the measured datum, $z$, for the two candidate hypothesis.\n",
    "A Bayesian framework is incorporated, so we arbitrarily can choose \"synthetic\" as the null hypothesis and \"natural\" as the test hypothesis.\n",
    "In the Bayesian framework, the likelihood ratio is as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{Pr(H_0)}{Pr(H_1)} \\lessgtr \\frac{p_0(z)}{p_1(z)}\n",
    "\\end{equation}\n",
    "\n",
    "$Pr(H_i)$ indicates the probability of a hypothesis $H_i$ and $p_i(z)$ is the associated probability density function for the test statistic.\n",
    "For simplicity, synthetic and natural images are treated as equally likely, so the likelihood ratio is compared to one.\n",
    "The following figure shows the likelihood ratios for the synthetic and natural image sets.\n",
    "The likelihood threshold based on the priors is plotted for comparison.\n",
    "Clearly, the test is conservative with respect to synthetic images, and fails to detect all the natural scenes.\n",
    "No doubt better performance could be obtained using larger training sets and a more refined edge detection scheme.\n",
    "\n",
    "<img style=\"float: left;\" src=\"img3.png\">\n",
    "\n",
    "Using equal priors, there are zero false positives out of ninety-nine synthetic images and seven false negatives out of fifty-six natural scenes.\n",
    "This is a total error rate of just about 4.5% for the whole data set.\n",
    "It should be noted that the test set includes the images used in training the statistics for the Cauchy distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II) Grayscale Intensity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach: \n",
    "Synthetic and natural images are expected to have measureably different intensity distributions.\n",
    "Natural images, in general, are expected to have a smoother distribution of intensity values, while synthetic images have a sharper distributions.\n",
    "The strategy is to compute the sum of the difference between adjacent bins of the grayscale histogram, which should be a function of histogram smoothness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple illustration of natural image histogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Image_codeline3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitted  Image Histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following image shows a histogram of the grayscale intensity metric for the natural images.\n",
    "A normal distribution fit to the data is shown, and is used to approximate the distribution for computing the likelihood ratio.\n",
    "\n",
    "<img src=\"Image_codeline9.png\">\n",
    "\n",
    "The following image shows a histogram of the grayscale intensity metric for the synthetic images.\n",
    "\n",
    "<img src=\"Image_codeline9(2).png\">\n",
    "\n",
    "The final image shows the distributions for the natural images (blue) and the synthetic images (green).\n",
    "The continuous PDFs are used in a likelihood ratio test for new images in the same fashion introduced in Section 1.\n",
    "\n",
    "<img src=\"Image_codeline9(3).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result\n",
    "\n",
    "The proposed likelihood ratio test based on grayscale histogram difference is evaluated on the available image sets.\n",
    "In this section, the null hypothesis is a natural image, so the false positive rate indicates the fraction of images incorrectly classified as synthetic.\n",
    "\n",
    "* False Positive Rate: 9.6%\n",
    "* False Negative Rate: 26.5%\n",
    "* Total Error Rate: 18%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III) RGB Peak Density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "\n",
    "The intensity-based metric of Section 2 performs reasonably well.\n",
    "However, it seems plausble that better performance could be achieved by leveraging the color information of the images.\n",
    "This is particularly reasonable, since the synthetic images are all of the same 3D models.\n",
    "The limitation with this approach is that it is not practical to develop a model that is predictive for all of the natural images.\n",
    "\n",
    "The feature of interest is essentially the same metric as in Section 2, but for all three color channels (R,G,B) simultaneously.\n",
    "Essentially it is desired to look at the sharpness of all three color histograms.\n",
    "The feature metric is the sum of the squared maximum difference within the subhistograms.\n",
    "Taking $\\Delta_i$ to be the difference between adjacent histogram bins for color channel $i$, the metric is written as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "z = \\max ( \\Delta_{red} )^2 + \\max ( \\Delta_{green} )^2 + \\max ( \\Delta_{blue} )^2\n",
    "\\end{equation}\n",
    "\n",
    "To motivate the use of this metric, consider the following examples of red color histograms from one natural image and one scene.\n",
    "\n",
    "#### Example of Scene RGB Histogram:\n",
    "\n",
    "<img src=\"sceneRhist.png\">\n",
    "\n",
    "#### Example of Synthetic RGB Histogram:\n",
    "\n",
    "<img src=\"synthRhist.png\">\n",
    "\n",
    "Clearly, for the particular case considered, the peak of the red channel histogram is much \"sharper\" for the synthetic image.\n",
    "The following sub-section presents representative training histograms and summarizes performance of a likelihood-ratio-based image detector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine Threshold Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Method Explanation:\n",
    "\n",
    "As in the previous sections, the strategy is to fit a continuous distribution to a training subset of the available images.\n",
    "For brevity, much of this development is skimmed over and only the final results are presented.\n",
    "Based on histograms of the feature metrics for synthetic and natural image sets, Cauchy distributions were selected as models for computing the likelihood ratio for the two image categories.\n",
    "As may be expected from the histogram plot that follows, the distributions are quite distinctive.\n",
    "The natural scenes are essentially concentrated around a metric value of zero, indicating that the peak intensity sharpness is extremely low.\n",
    "The synthetic images have a peak further to the right on the plot.\n",
    "\n",
    "<img src=\"metrichistogram.png\">\n",
    "\n",
    "The fitted Cauchy distributions are shown in the following plot.\n",
    "In addition to the distributions, this plot shows the log-likelihood ratio on a second axis as a function of the peak sharpness metric.\n",
    "The likelihood ratio is monotone and its zero crossing is at $z \\approx 3.8\\times 10^5$.\n",
    "\n",
    "<img src=\"fitteddists.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "For brevity, a detailed analysis of performance is omitted.\n",
    "In the analysis, the null hypothesis $H_0$ is taken to be a natural scene.\n",
    "The performance metrics are summarized below.\n",
    "As can be seen, the rate of false positives is fairly high with this metric.\n",
    "This is due to synthetic images that lack a well-defined sharp peak.\n",
    "\n",
    "* False Positive Rate:    0.415\n",
    "* False Negative Rate:    0.163\n",
    "* Overall Error Rate:     0.289\n",
    "\n",
    "This section has considered the use of the RGB peak signal sharpness to discriminate synthetic and natural images.\n",
    "The following section considers a metric of image blur derived from the published literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV) Image Sharpness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach\n",
    "\n",
    "We shall use the following principle to design a statistical model in order to perform the binary detection of images: \"A sharper good quality image will have higher number of high frequency components compared to a blurred image.\"\n",
    "As the synthetic (computer generated) images have sharper edges when compared to the natural (photographic/scenic) images, the synthetic image shall contain a greater number of high frequency components when compared to the natural images.\n",
    "The image blur metric is derived from the paper by De and Masilamani [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Explanation\n",
    "\n",
    "Ref. [1] defines an image sharpness metric that is adapted in the present work to differentiate the sharper synthetic images from natural images.\n",
    "The sharpness metric based on the 2D fast Fourier transform (FFT) of the image.\n",
    "The metric is the fraction of pixels that exceed 1/1000th of the peak absolute FFT value.\n",
    "Characteristic values of the metric for synthetic and natural images are computed from training sets, as follows.\n",
    "20 images from each of the sets are taken as training data.\n",
    "The following figures show histograms for the image sharpness metric in the synthetic and natural image training sets:\n",
    "\n",
    "<img src=\"index.png\">\n",
    "\n",
    "<img src=\"index2.png\">\n",
    "\n",
    "\n",
    "The histograms are treated as probability mass functions and used in a likelihood ratio test as in previous sections.\n",
    "For these distributions, the likelihood ratio is monotonic and can be converted to a threshold on the sharpness metric.\n",
    "That threshold is taken as $0.71$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume our test hypothesis ($H_1$) to be the detection of computer generated images, the probabilities of false alarm and miss detection are given as follows:\n",
    "\n",
    "* The Rate of False Alarms is: 0.057\n",
    "* The Rate of Missed Detections is: 0.028\n",
    "* The Accuracy of the Detector is: 91.428 %\n",
    "\n",
    "This section has presented an image sharpness metric for discriminating synthetic and natural images.\n",
    "The following section summarizes the performance of the four detectors considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "### I) Edge Detection\n",
    "#### Total Error Rate: 4.5%\n",
    "\n",
    "### II) Grayscale Intensity\n",
    "#### Total Error Rate: 18.0%\n",
    "\n",
    "### III) RGB Peak Density\n",
    "####  Total Error Rate: 27.0%\n",
    "\n",
    "### IV) Image Sharpness\n",
    "####  Total Error Rate: 8.5%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Intensity and RGB methods (Sections 2 and 3) work well for detecting computer generated images, which are dominated by a few colors. However, it is difficult to generalize a model for natural scenes, which limits the performance of these methods.\n",
    "* Analyzing this problem within the Fourier Domain space seems more robust than analyzing color or intensity sharpness. It is also thought that this approach should be less prone to overfitting than the intensity-based metrics. \n",
    "* The lowest overall error rate is achieved with the edge detection approach. This technique is perhaps more prone to overfitting than the FFT approach, since it depends on the existence of a model that predicts the presence of edges in natural scenes. Certain scenes like portraits or still life with strong lighting may appear more like synthetic images using this approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## References\n",
    "[1] D. Kanjar, V. Masilamani. \"Image sharpness measure for blurred images in frequency domain.\" Procedia Eng, 64 (2013), pp. 149â€“158."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
