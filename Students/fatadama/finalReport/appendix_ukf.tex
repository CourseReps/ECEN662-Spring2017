\section{Unscented Kalman Filter: algorithm summary}

The Unscented Kalman Filter algorithm is originally given by Ref. \cite{wan2000}. The notation of the following equations is motivated by Ref. \cite{woodbury2015} as well.
Again, discrete nonlinear dynamics with discrete nonlinear measurements are considered.
Define an augmented state vector as follows:

\begin{equation}
\vecin{x^a}{k} = \begin{bmatrix}
\vecin{x}{k} \\\vecin{w}{k} \\\vecin{v}{k}
\end{bmatrix} \equiv \begin{bmatrix}
\vecin{x^x}{k} \\ \vecin{x^w}{k} \\\vecin{x^v}{k}
\end{bmatrix}
\end{equation}

Note the use of the superscripts $x,w,v$ to denote the entries of the augmented state vector corresponding to the state, process noise, and measurement noise terms, respectively; this notation will be convenient later.
Similarly, define the associated sugmented state covariance matrix:

\begin{equation}
\ten{P^a}(k) \equiv \begin{bmatrix}
\P(k) & 0 & 0\\
0 & \ten{Q}(k) & 0\\
0 & 0 & \ten{R}(k)
\end{bmatrix}
\end{equation}

Denote the length of the augmented state vector as $L$.
Now, define an $L \times (2L+1)$ array of \textit{sigma points}, $\ten{\chi}(k)$.
The $i$th column of $\ten{\chi}(k)$ is defined as follows:

\begin{equation}
\ten{\chi_i}(k)
\begin{cases}
\vecin{x^a}{k} & i = 1\\
\vecin{x^a}{k} + \sqrt{(L+\lambda)}\sqrt{\ten{P^a_i}(k)} & i = 2,\dots,L+1\\
\vecin{x^a}{k} - \sqrt{(L+\lambda)}\sqrt{\ten{P^a_i}(k)} & i = L+2,\dots,2L+1
\end{cases}
\end{equation}

Here, $\sqrt{\ten{P^a_i}(k)}$ is used to denote the $i$th column of the matrix square root of the augmented covariance matrix.
Observe that the sigma point matrix includes perturbed values of the state estimate \textit{as well as of the process and measurement noise vectors.}
Those process and measurement noise elements in the sigma points are passed into the governing dynamics.
Note the presence of $\lambda$, a scalar tuning parameter, that is summarized in the next subsection.
Each column of the sigma point array is a point approximation to the prior, and is passed through the prediction equation as follows to produce a \textit{prediction sigma point array} $\ten{\chi^-}(k+1)$, whose columns are determined as follows:

\begin{equation}
\ten{\chi^-_i}(k+1) = \f(\ten{\chi^x_i}(k),\vecin{u}{k}) + \G(k) \ten{\chi^w_i}(k), i = 1, \dots, 2L+1
\end{equation}

The state and covariance prediction approximations are weighted sums of the propagated sigma points. Denote the mean and covariance weight vectors by $\vec{W^m}$ and $\vec{W^c}$; they are defined separately in the sub-section ``Tuning parameters and weights.''

\begin{equation}
\vecin{\hat{x}^-}{k+1} = \sum_{i=1}^{2L+1} \vec{W^m}(i) \ten{\chi^{x-}_i}(k+1)
\end{equation}
\begin{equation}
\P^-(k+1) = \sum_{i=1}^{2L+1} \vec{W^c}(i) (\ten{\chi^{x-}_i}(k+1)-\vecin{\hat{x}^-}{k+1})(\ten{\chi^{x-}_i}(k+1)-\vecin{\hat{x}^-}{k+1})^T
\end{equation}

For the measurement update step, it is convenient to switch to the index $k$.
Now, the prediction sigma points are each passed through the measurement model, leading to the \textit{measurement sigma point array} $\ten{Y}$:

\begin{equation}
\ten{Y_i}(k) = \h(\ten{\chi^{x-}_i}(k)) + \ten{\chi^{v-}_i}(k)
\end{equation}

The measurement expectation, measurement variance, and prediction-measurement covariance are then approximated by weighted sums:
\begin{align}
\vecin{\hat{y}}{k} \equiv \sum_{i=1}^{2L+1} \vec{W^m}(i) \ten{Y_i}(k) \\
\ten{P_{yy}}(k) \equiv \sum_{i=1}^{2L+1} \vec{W^c}(i) (\ten{Y_i}(k)-\vecin{\hat{y}}{k})(\ten{Y_i}(k)-\vecin{\hat{y}}{k})^T
\\
\ten{P_{xy}}(k) \equiv \sum_{i=1}^{2L+1} \vec{W^c}(i) (\ten{\chi^{x-}_i}(k)-\vecin{\hat{x}^-}{k})(\ten{Y_i}(k)-\vecin{\hat{y}}{k})^T
\end{align}

The Kalman gain is then computed from the variance terms, and the posterior state estimate and variance follow:
\begin{align}
\ten{K}(k) \equiv \ten{P_{xy}}\ten{P_{yy}}^{-1}
\\
\vecin{\hat{x}}{k} = \vecin{\hat{x}^-}{k} + \ten{K}(k)(\vecin{\tilde{y}}{k}-\vecin{\tilde{y}}{k})
\\
\P(k) = \P^-(k) - \ten{K}(k)\ten{P_{yy}}^{-1} \ten{K}^T(k)
\end{align}

The posterior estimate and variance are used to create a new augmented state vector and sigma point matrix, and the process repeats.
The next subsection defines the weights and tuning parameters used in the UKF.

\subsubsection{Tuning parameters and weights}

The mean and covariance weights are selected so that the weighted sums of the prior sigma points is the prior mean and covariance.
The weights are defined as follows:

\begin{equation}
\vec{W^m}(i) = \begin{cases}
\frac{\lambda}{L + \lambda} & i = 1 \\
\frac{1}{2(L+\lambda)} & i = 2, \dots, 2L+1
\end{cases}
\end{equation}
\begin{equation}
\vec{W^c}(i) = \begin{cases}
\frac{\lambda}{L + \lambda}+(1-\alpha^2+\beta) & i = 1 \\
\frac{1}{2(L+\lambda)} & i = 2, \dots, 2L+1
\end{cases}
\end{equation}

$\beta$ is used to incorporate prior knowledge of the ``true'' distribution; $\beta = 2$ is optimal for Gaussian priors.

$\lambda$ is defined as follows.
$\alpha$ determines the spread of the sigma points about the mean and is generally set to a small value, say $10^{-3}$.
$\kappa$ is a ``secondary scaling parameter,'' and is commonly set to zero [2].

\begin{equation}
\lambda = \alpha^2(L+\kappa)-L
\end{equation}